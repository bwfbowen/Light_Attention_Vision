{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install coax\n",
    "!pip install flax\n",
    "!pip install haiku \n",
    "!pip install optax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs on google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gym\n",
    "import jax\n",
    "import coax\n",
    "import haiku as hk\n",
    "import jax.numpy as jnp\n",
    "from optax import adam\n",
    "\n",
    "from google.colab import drive\n",
    "from light_vision_attention import VisionAttn\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# set some env vars\n",
    "os.environ.setdefault('JAX_PLATFORM_NAME', 'gpu')     # tell JAX to use GPU\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.9'  # don't use all gpu mem\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'              # tell XLA to be quiet\n",
    "\n",
    "model_names = ['dqn_fast_attn', 'dqn_conv', 'dqn_regular_attn']\n",
    "env_names = ['TennisNoFrameskip-v4', 'PongNoFrameskip-v4', 'BreakoutNoFrameskip-v4', 'AsteroidsNoFrameskip-v4']\n",
    "\n",
    "model_name = model_names[0]\n",
    "env_name = env_names[0]\n",
    "\n",
    "config = {\n",
    "    'name': model_name,\n",
    "    'TMAX': 3000000,\n",
    "    'temperature': 0.015,\n",
    "    'ER_beta': 0.001,\n",
    "    'lr': 3e-4,\n",
    "    'Nstep_n': 5,\n",
    "    'Nstep_gamma': 0.99,\n",
    "    'RB_capacity': 1000000,\n",
    "    'RB_alpha': 0.6,\n",
    "    'RB_warmup': 50000,\n",
    "    'batch_size': 256,\n",
    "    'save_T_init': 50000,\n",
    "    'save_T_period': 50000,\n",
    "    'soft_update_tau': 1,\n",
    "    'update_freq': 10000,\n",
    "    'learn_freq': 4,\n",
    "    'env_name': env_name,\n",
    "    'num_frames': 3,\n",
    "    'max_episode_steps': 108000 // 3,\n",
    "    'tensorboard_dir': 'drive/MyDrive/data/tensorboard/'\n",
    "}\n",
    "\n",
    "\n",
    "# env with preprocessing\n",
    "def make_env(config):\n",
    "  name = config['name']\n",
    "  # env with preprocessing\n",
    "  env = gym.make(config['env_name'], render_mode='rgb_array')\n",
    "  env = gym.wrappers.AtariPreprocessing(env)\n",
    "  env = coax.wrappers.FrameStacking(env, num_frames=config['num_frames'])\n",
    "  env = gym.wrappers.TimeLimit(env, max_episode_steps=config['max_episode_steps'])\n",
    "  env = coax.wrappers.TrainMonitor(env, \n",
    "                                    name=name, \n",
    "                                    tensorboard_dir=os.path.join(config['tensorboard_dir'],\n",
    "                                                                f\"{name}_{config['env_name']}\"))\n",
    "  return env\n",
    "\n",
    "env = make_env(config)\n",
    "\n",
    "if config['name'] == 'dqn_conv':\n",
    "  def func(S, is_training):\n",
    "      \"\"\" type-2 q-function: s -> q(s,.) \"\"\"\n",
    "      seq = hk.Sequential((\n",
    "          coax.utils.diff_transform,\n",
    "          hk.Conv2D(32, kernel_shape=8, stride=4, padding='VALID'), jax.nn.relu,\n",
    "          hk.Conv2D(64, kernel_shape=4, stride=2, padding='VALID'), jax.nn.relu,\n",
    "          hk.Conv2D(64, kernel_shape=3, stride=1, padding='VALID'), jax.nn.relu,\n",
    "          hk.Flatten(),\n",
    "          hk.Linear(256), jax.nn.relu,\n",
    "          hk.Linear(env.action_space.n, w_init=jnp.zeros),\n",
    "      ))\n",
    "      X = jnp.stack(S, axis=-1) / 255.  # stack frames\n",
    "      return seq(X)\n",
    "\n",
    "elif config['name'] == 'dqn_fast_attn':\n",
    "  def func(S, is_training):\n",
    "      \"\"\" type-2 q-function: s -> q(s,.) \"\"\"\n",
    "      seq = hk.Sequential([\n",
    "          coax.utils.diff_transform,\n",
    "          hk.Conv2D(32, kernel_shape=8, stride=4, padding='VALID'), jax.nn.relu,\n",
    "          hk.Conv2D(64, kernel_shape=4, stride=2, padding='VALID'), jax.nn.relu,\n",
    "          hk.Conv2D(64, kernel_shape=3, stride=1, padding='VALID'), jax.nn.relu,\n",
    "          VisionAttn(embed_dim=64,\n",
    "                     hidden_dim=128,\n",
    "                     num_heads=8,\n",
    "                     num_layers=2,\n",
    "                     num_patches=64,\n",
    "                     dropout_prob=0.1,\n",
    "                     use_fask_attn=True,\n",
    "                     name='fast_attn'),\n",
    "          hk.Flatten(),\n",
    "          hk.Linear(256), jax.nn.relu,\n",
    "          hk.Linear(env.action_space.n, w_init=jnp.zeros),          \n",
    "      ])\n",
    "      X = jnp.stack(S, axis=-1) / 255.  # stack frames\n",
    "      return seq(X)\n",
    "\n",
    "elif config['name'] == 'dqn_regular_attn':\n",
    "  def func(S, is_training):\n",
    "      \"\"\" type-2 q-function: s -> q(s,.) \"\"\"\n",
    "      seq = hk.Sequential([\n",
    "          coax.utils.diff_transform,\n",
    "          hk.Conv2D(32, kernel_shape=8, stride=4, padding='VALID'), jax.nn.relu,\n",
    "          hk.Conv2D(64, kernel_shape=4, stride=2, padding='VALID'), jax.nn.relu,\n",
    "          hk.Conv2D(64, kernel_shape=3, stride=1, padding='VALID'), jax.nn.relu,\n",
    "          VisionAttn(embed_dim=64,\n",
    "                     hidden_dim=128,\n",
    "                     num_heads=8,\n",
    "                     num_layers=2,\n",
    "                     num_patches=64,\n",
    "                     dropout_prob=0.1,\n",
    "                     use_fask_attn=False,\n",
    "                     name='regular_attn'),\n",
    "          hk.Flatten(),\n",
    "          hk.Linear(256), jax.nn.relu,\n",
    "          hk.Linear(env.action_space.n, w_init=jnp.zeros),          \n",
    "      ])\n",
    "      X = jnp.stack(S, axis=-1) / 255.  # stack frames\n",
    "      return seq(X)\n",
    "\n",
    "def dqn_train(func, config):\n",
    "    ########################\n",
    "    # Environment\n",
    "    ########################\n",
    "    # the name of this training script\n",
    "    name = config['name']\n",
    "\n",
    "    # env with preprocessing\n",
    "    env = make_env(config)\n",
    "    ########################\n",
    "    # Agent\n",
    "    ########################\n",
    "    # function approximators\n",
    "    # function approximator\n",
    "    q = coax.Q(func, env)\n",
    "    pi = coax.BoltzmannPolicy(q, temperature=config['temperature'])  # <--- different from standard DQN (Îµ-greedy)\n",
    "\n",
    "    # target network\n",
    "    q_targ = q.copy()\n",
    "\n",
    "    # updater\n",
    "    qlearning = coax.td_learning.QLearning(q, q_targ=q_targ, optimizer=adam(config['lr']))\n",
    "\n",
    "    # reward tracer and replay buffer\n",
    "    tracer = coax.reward_tracing.NStep(n=config['Nstep_n'], gamma=config['Nstep_gamma'])\n",
    "    buffer = coax.experience_replay.PrioritizedReplayBuffer(capacity=config['RB_capacity'], alpha=config['RB_alpha'])\n",
    "\n",
    "    # schedule for the PER beta hyperparameter\n",
    "    beta = coax.utils.StepwiseLinearFunction((0, 0.4), (1000000, 1))\n",
    "\n",
    "    while env.T < config['TMAX']:\n",
    "        s, info = env.reset()\n",
    "        buffer.beta = beta(env.T)\n",
    "\n",
    "        for t in range(env.spec.max_episode_steps):\n",
    "            a = pi(s)\n",
    "            s_next, r, done, truncated, info = env.step(a)\n",
    "\n",
    "            # trace rewards and add transition to replay buffer\n",
    "            tracer.add(s, a, r, done or truncated)\n",
    "            while tracer:\n",
    "                transition = tracer.pop()\n",
    "                buffer.add(transition, qlearning.td_error(transition))\n",
    "\n",
    "            # learn\n",
    "            if env.T % config['learn_freq'] == 0 and len(buffer) > config['RB_warmup']:  # buffer warm-up\n",
    "                transition_batch = buffer.sample(batch_size=config['batch_size'])\n",
    "                metrics, td_error = qlearning.update(transition_batch, return_td_error=True)\n",
    "                buffer.update(transition_batch.idx, td_error)\n",
    "                env.record_metrics(metrics)\n",
    "\n",
    "            if env.T % config['update_freq'] == 0:\n",
    "                q_targ.soft_update(q, tau=config['soft_update_tau'])\n",
    "\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "            s = s_next\n",
    "\n",
    "        # generate an animated GIF to see what's going on\n",
    "        if env.period(name='generate_gif', T_period=config['save_T_period']) and env.T > config['save_T_init']:\n",
    "            coax.utils.dump(pi, 'pi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_train(func, config)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
