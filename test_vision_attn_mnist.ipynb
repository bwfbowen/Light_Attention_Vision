{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install haiku\n",
    "!pip install --upgrade jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import haiku as hk \n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from jax import numpy as jnp\n",
    "\n",
    "from src.light_vision_attention import VisionAttn\n",
    "\n",
    "X, Y = datasets.load_digits(n_class=10, return_X_y=True, as_frame=False)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, stratify=Y, random_state=123)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = jnp.array(X_train, dtype=jnp.float32),\\\n",
    "                                   jnp.array(X_test, dtype=jnp.float32),\\\n",
    "                                   jnp.array(Y_train, dtype=jnp.float32),\\\n",
    "                                   jnp.array(Y_test, dtype=jnp.float32)\n",
    "\n",
    "samples, features = X_train.shape\n",
    "classes = jnp.unique(Y_test)\n",
    "\n",
    "X_train = X_train.reshape(-1, 8, 8, 1) / 255\n",
    "X_test = X_test.reshape(-1, 8, 8, 1) / 255\n",
    "\n",
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VisionAttnFn(x):\n",
    "    van = VisionAttn(32, 64, 4, 2, 4, 16, 0.2, use_fask_attn=False)\n",
    "    dense = hk.Linear(len(classes))\n",
    "    flatten = hk.Flatten()\n",
    "    return jax.nn.softmax(dense(flatten(van(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import value_and_grad\n",
    "\n",
    "rng = jax.random.PRNGKey(42) ## Reproducibility ## Initializes model with same weights each time.\n",
    "\n",
    "# model = hk.transform(ConvNet)\n",
    "model = hk.transform(VisionAttnFn)\n",
    "params = model.init(rng, X_train[:5])\n",
    "epochs = 25\n",
    "batch_size = 256\n",
    "learning_rate = jnp.array(1/1e4)\n",
    "\n",
    "def CrossEntropyLoss(weights, input_data, actual):\n",
    "    preds = model.apply(weights, rng, input_data)\n",
    "    one_hot_actual = jax.nn.one_hot(actual, num_classes=len(classes))\n",
    "    log_preds = jnp.log(preds)\n",
    "    return - jnp.sum(one_hot_actual * log_preds)\n",
    "\n",
    "def UpdateWeights(weights,gradients):\n",
    "    return weights - learning_rate * gradients\n",
    "\n",
    "    \n",
    "for i in range(1, epochs+1):\n",
    "    batches = jnp.arange((X_train.shape[0]//batch_size)+1) ### Batch Indices\n",
    "\n",
    "    losses = [] ## Record loss of each batch\n",
    "    for batch in batches:\n",
    "        if batch != batches[-1]:\n",
    "            start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
    "        else:\n",
    "            start, end = int(batch*batch_size), None\n",
    "\n",
    "        X_batch, Y_batch = X_train[start:end], Y_train[start:end] ## Single batch of data\n",
    "\n",
    "        loss, param_grads = value_and_grad(CrossEntropyLoss)(params, X_batch, Y_batch)\n",
    "        #print(param_grads)\n",
    "        params = jax.tree_map(UpdateWeights, params, param_grads) ## Update Params\n",
    "        losses.append(loss) ## Record Loss\n",
    "\n",
    "    print(\"CrossEntropy Loss : {:.3f}\".format(jnp.array(losses).mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39-sci')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9ead6539b7f91ca401f595897742956cc6fab5542803c2b8a5c0cf3ced7ed58"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
