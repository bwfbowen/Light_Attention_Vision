# Light_Attention_Vision
1. INTRODUCTION AND RELATED WORK![image](https://user-images.githubusercontent.com/104526323/208587128-61b23376-be06-4e9f-9de9-51e15e634387.png)
Q-learning algorithm with convolutional neural network, whose input is raw pixels has demonstrated success on several Atari games decades ago (Mnih et al., 2013). The neural network plays the role of both state representation and policy π:S→A. Thus, the ability to identify the latent state from raw observations and learn the proper policy from state space to actions are the critical ingredient of the performance of different NN architecture. Transformers (Vaswani et al., 2017) have become SOTA in different areas ranging from natural language processing (NLP), time series prediction, to image generation. The success of Transformers rely on the trainable attention mechanism which identifies complex depencies between elements of each input sequence. Despite the power that attention mechanism processes complex information, attention mechanism is expensive for the fact that it scales quadratically with the length L of the input sequence. Performers ( Choromanski et al., 2020) improve the regular attention with Fast Attention Via positive Orthogonal Random features (FAVOR+) mechanism, which is provably accurate and only takes linear space and time complexity. For this project, I use Deep Q Networks (DQN) with different NN models to train agents directly from raw pixels and compare the performance.![image](https://user-images.githubusercontent.com/104526323/208587167-2bcd721f-6d6e-4a5a-aa5f-d430b6eea2fd.png)

Transformers were proposed originally to process sets instead of sequence since it produces the same output if the input is permuted. To apply Transformers to sequences, a positional encoding is added. Pre-Layer Normalization (Xiong et al., 2020) is used(Figure 1), which is a version of the Transformer that applies Layer Normalization first in each residual block. Pre-LN is more stable for training Transformers, which supports better gradient flow and removes the necessity of a warm-up stage.
For the implementation, the Feed Forward block is two fully connected layers with GELU activation. The Feed Forward block introduces much more parameters while the gain is uncertain. Therefore, simplified version of Transformer Encoder with simply attention blocks is tested against the full Transformer Encoder.
![image](https://user-images.githubusercontent.com/104526323/208587077-b10c2904-517d-4b23-b676-6b6033b87a49.png)

<img width="484" alt="image" src="https://user-images.githubusercontent.com/104526323/208586920-2ab05f8a-a2e0-4ca5-acdb-f35fd3d8d4e4.png">
